---
title: "ESM244 Lab 8: GitHubbing, Spatial Point Pattern Analysis, and PCA"
author: "Danielle Bovenberg"
date: "7 March 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##0. Loading packages

```{r packages}
library(tidyverse)
library(sf)
library(tmap)
library(leaflet)
library(spatstat)
library(maptools)
library(corrplot)

```

##1. About Github

Clone information locally
edit information
commit changes
then upload to github
The sequence is (1) stage, (2) commit, (3) push. 


```{r}

df <- oil_spills %>% 
  filter(`Accident State` == "TX" & `Accident Year` < 2017) %>% 
  group_by(`Accident Year`) %>% 
  summarise(Loss = sum(`Net Loss (Barrels)`))

colnames(df) <- c("Year","Loss")

ggplot(df, aes(x = Year, y = Loss)) +
  geom_col() +
  xlab("Year") +
  ylab("Texas Oil Spill Losses (Barrels)") 


```


##2. Making a Leaflet plot of spill locations in Texas in 2016

```{r texas}

oil_spills <- read_csv("oil_spills.csv")

df_loc <- oil_spills %>% 
  filter(`Accident State` == "TX" & `Accident Year` == 2015) %>% 
  select(Latitude, Longitude, `Net Loss (Barrels)`)

colnames(df_loc) <- c("latitude", "longitude", "net_loss")

```
R doesn't yet recognize that this is spacial information. 
Next, we're going to convert the dataframe into "simple feature"

```{r}

oil_sf <- st_as_sf(df_loc, coords = c("longitude", "latitude"), crs = 4326)

```

The class of oil_sf is now simple features spacial data. You can treat it as a dataframe, but with sticky geometry. it retains the spatial geometry. 

```{r}

leaflet(oil_sf) %>% 
  addTiles() %>% 
  addMarkers()

```

## 3. tmap plot with the Texas state shapefile
Next is point pattern analysis. We need an actual shape file (a bounding window) to do point pattern analysis, not just a leaflet plot. 

First, let's insert a new code chunk to load the files.

```{r}

states <- st_read(dsn = ".", layer = "states") #the layer I want to read in is everything that starts with a prefix called 'states'. 

tex_border <- states %>% 
  filter(STATE_NAME == "Texas") %>% 
  st_transform(4326) # assigning a crs (coordinate reference system)

st_crs(tex_border)

plot(tex_border)

tm_shape(tex_border) +
  tm_polygons() +
  tm_shape(oil_sf) +
  tm_dots(size = 0.3)

```
I can treat "states" as a dataframe, and it will keep the geometry of the poligons. 

I'm only interested in the TX outline.

Does this point pattern follow CSR? Completeley spatially random. 

###3.1 Convert the data to spatial points patterns (combination of point data and the bounding window)

```{r}

spill_sp <- as(oil_sf, "Spatial") #converting from simple features back to data frame
spill_ppp <- as(spill_sp, "ppp") #ppp for point pattern analysis. 
class(spill_ppp)

tx_sp <- as(tex_border, "Spatial")
tx_owin <- as(tx_sp, "owin") # window of observation, of class "owin"

all_ppp <- ppp(spill_ppp$x, spill_ppp$y, window = tx_owin)

```

The function ppp() omits points that don't align for the point pattern and the window that you set. What these last four steps have done, is putting the spatial data in a format that R recognizes as points and window, and then puts it into its brain as one entity. Once you do that, we can use existing pp analysis functions. 

### Kernel density plot

```{r}

plot(density(all_ppp, sigma = 0.4)) #radius of bubbles is 0.4
# density plots are easy to make and also to manipulate. 
# pick a meaningful way to decide what sigma (your bandwidth) will be. 

```


### Quadrant test for spatial evenness

Are oil spills evenly distributed throughout the state? 

```{r}

oil_qt <- quadrat.test(all_ppp, nx = 5, ny = 5) #arbitrary choice of quadrant amount
oil_qt

plot(all_ppp)
plot(oil_qt, add = TRUE, cex = 0.4) # run these two lines together. 
# the plot shows you what the quadrats are and associated counts
```

We see a very small p-value. It tests the null hypothesis of CSR (following a poisson distribution). We can reject the null hypothesis. The data is not evenly distributed.

The plot tells us that if this data was truly distributed evenly, then the number of events expected in middle area is 14.5. For partial quadrats it figures out a proportional number of counts. 

If data is more clustered than CSR, we would expect the nearest neighbors to be closer together, on average. There are two ways to evaluate this. 

What proportion of points have their nearest neighbor within a distance r? Clustered data has a higher proportion. If the distribution is smoother, there is a lower proportion. The g-function looks at this. 

### G-Function for nearest neighbor analysis

Simulate CSR data. 

```{r}

r <- seq(0,1, by = 0.01) # the lag

oil_gfun <- envelope(all_ppp, fun = Gest, r = r, nsim = 100) # for L or K, it's Lest and Kest functions

ggplot(oil_gfun, aes(x = r, y = obs)) +
  geom_line(color = "black") +
  geom_line(aes(x = r, y = theo), color = "red")


```
oil_gfun gives you a lag 'r' for each of the values that range from 0 to 1 by 0.1 increments (as we created). For each of those values, the 'observed' column is calculated from our data. 'Theo' is theoretical based on simulations. 

The graph: our observed data has a higher proportion of point pairs with nearest neighbors at shorter distances compared to CSR data. On average, our data points have a nearest neighbor that is closer than we would expect if the data was completely randomly distributed. 

Next, how about concentrations around every point in our dataset? For this, we use the L-function. 

### Nearest neighbor using the L-function (Ripley's K, standardized)

```{r}

r2 <- seq(0, 3, by = 0.5)
oil_lfun <- envelope(all_ppp, fun = Lest, r = r2, nsim = 20, global = TRUE)


ggplot(oil_lfun, aes(x = r2, y = obs)) +
  geom_line(color = "black") +
  geom_line(aes(x = r2, y = theo), color = "blue")

```
Every sinlge point in space and makes increasing bubbles around it until it incorporates all points that exist. 

